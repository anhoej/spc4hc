---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Basic Statistical Concepts {#stat-concepts}

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.asp = 3/5,
                      fig.width = 10,
                      dev     = 'svg')

source('R/load_data.R', local = knitr::knit_global())
```

## Data types

Data can be classified into several types, with the most common being categorical and numerical data.

### Categorical data

Categorical data, also known as qualitative data, represent characteristics that cannot be meaningfully measured on a numerical scale, for example [red, green, blue], [single, married, divorced, widowed] or [low, medium, high]. Categorical data cannot be directly analysed using arithmetic operations. What is, for example, the sum of red, green and blue?

Categorical data can be divided into:

* Nominal data: Categories without a natural order, e.g. [red, green, blue] or [single, married, divorced, widowed].

* Ordinal data: Categories with a meaningful order, but without consistent intervals between them, e.g. [low < medium < high].

If categorical data consist of only two possible values -- such as yes/no, true/false or pregnant/not pregnant -- they are referred to as binary or binomial data.

### Numerical data

Numerical data, also known as quantitative data, consist of values that represent measurable quantities. These data can be meaningfully analysed using arithmetic operations such as addition or subtraction.

Numerical data fall in two main subtypes:

* Discrete data: Distinct, separate values that result from counting and are always expressed as whole numbers (including zero). Examples include number of pregnancies, pressure ulcers and surgical complications.

* Continuous data: Values obtained through measurement that can take on any value within a given range, including decimals, fractions and negative numbers. Examples include height, weight, waiting time and blood pressure.

Just as categorical data can be converted into numbers through counting, numerical data can be transformed into categories by grouping values into defined ranges â€” for example, converting exact blood pressure readings into hypertension categories such as normal, elevated, stage 1 hypertension, and stage 2 hypertension.

## Summarising categorical data

In this section we use the [Adverse events](#adverse-events) dataset (assigned to the variable 'ae').
 
Categorical data are most commonly summarised by counting the number of observations in each category, typically presented as frequencies or counts. In R, this can be easily done using the table() function.

```{r}
# count number of adverse events in each category
(ae.tbl <- table(ae$category))
```

Note that by counting we are effectively converting categorical data into numbers.

A useful visualisation of categorical data is the bar chart displaying either the frequencies or percentages in each category:

```{r stat-bar1, fig.cap='Bar chart of adverse event frequencies'}
# plot frequencies
barplot(ae.tbl)
```

```{r stat-bar2, fig.cap='Bar chart of adverse event proportions.'}
# plot proportions
barplot(prop.table(ae.tbl))
```

For binary data, a summary may simply involve calculating the proportion or percentage of one category relative to the total, for example the proportion of adverse events with fatal outcome:

```{r}
# proportion of fatal adverse events (severity = 'I').
mean(ae$severity == 'I')
```

## Summarising numerical data

For this section we use the [Robson group 1 births](#robson-group-1-births) and the [Renography doses](#renography-doses) datasets assigned to the births and reno variables respectively.

Numerical data are commonly summarised using three key characteristics: central tendency, distribution shape and spread (variation). In R, the summary() function provides a quick -- although limited -- overview of all three:

```{r}
# summarise the length of newborn babies
summary(births$length)
```

summary() is a generic function that behaves differently depending on the type of input. With numerical input, it returns a table displaying the minimum and maximum values, the three quartiles (1st, Median, 3rd), the mean and the number of NA values (if any).

A most useful visualisation of the distribution of continuous data is the histogram:

```{r stat-hist1, fig.cap = 'Histogram of birth lengths.'}
hist(births$length)
```

A histogram groups data into intervals (called bins) and shows how many values fall within each interval. It is similar to a bar chart, but the bars are touching to reflect the fact that the data are continuous.

Note that there are much more to histograms than meets the eye. For a deeper understanding and additional options, read the documentation (`?hist`).

### Centre

The central tendency of a dataset refers to the typical or central value around which the data points cluster. It represents the centre of the distribution. Common measures of central tendency include the mean and the median.

The **mean** is perhaps the better-known of the two. It is calculated by summing all the values and dividing by the total number of values. For example, the mean of the values [1,2,3] is (1 + 2 + 3) / 3 = 2.

The **median** -- often overlooked but equally important -- is the middle value when the data are sorted in order. For example, given the values [2,1,3], the median is 2 (remember to sort first). If there is an even number of values, the median is the mean of the two central values; for instance, for [1,2,3,4], the median is 2.5.

In the previous examples, the mean and the median are identical. This occurs when the data are perfectly symmetrical. However, when the data are skewed -- that is, spread unevenly on either side of the centre -- the mean and median can differ significantly. For example, consider the values [1,2,999]. The mean is 334, heavily influenced by the extreme value, whereas the median remains 2, effectively splitting the dataset in two equally sized halves.

Imagine a playground seesaw with a child and an adult seated at opposite ends. The mean corresponds to the balance point of the seesaw, shifting closer to the heavier adult, reflecting the weighted average of the two positions. In contrast, the median represents the midpoint of the plank itself, simply dividing the seesaw into halves regardless of weight.

The choice between using the mean or the median depends largely on the purpose of the analysis as well as the characteristics of the data distribution. 

The mean is useful when you want to consider every value in the dataset and obtain a measure that reflects the overall average, especially when the data are symmetrically distributed without extreme outliers.

On the other hand, the median is often preferred when the data are skewed or contain outliers, as it provides a measure of central tendency that is resistant to extreme values and better represents the "typical" observation in such cases. 
In SPC the median is particularly useful for runs analysis with run charts. Because the median reflects the midpoint of the data, there is an equal probability -- fifty-fifty -- that any individual data point will fall above or below it. This balance is crucial for runs analysis, as it ensures that the number and length of runs (consecutive points above or below the median) can be assessed against expected probabilities under random variation. Consequently, using the median helps accurately detect non-random patterns or shifts in the process regardless of the shape of data.

In our birth lengths data, the mean and the median are nearly identical -- 51.7 vs 52.0.

Understanding the shape and spread of the data helps inform which measure will provide the most meaningful summary. 

### Shape

The shape of a dataset refers to the overall pattern or distribution of values. Two key features that describe the shape of data are symmetry and modality.

**Symmetry** describes how evenly the values are distributed around the centre. In a perfectly symmetrical distribution the left and right sides are mirror images and the mean and the median are identical. 

When one side of the distribution extends further than the other, the data are said to be skewed. If the tail extends towards higher values, the distribution is right-skewed (positively skewed), and the mean is greater than median. If it extends towards lower values, it is left-skewed (negatively skewed), and the mean is less than the median.

Symmetry is also characterised by the mean being approximately centred between the minimum and maximum values.

Because the mean and median of birth length are very close, we may assume that the data are approximately symmetrical. However, neither the mean nor the median is well centred between the minimum and maximum values, suggesting that the distribution may still be slightly skewed or influenced by outliers at one end.

The histogram in Figure \@ref(fig:stat-hist1) provides a much clearer and more detailed picture of the shape of the data. The majority of values are symmetrically centred, but a few data points have unusually low values (birth length = 35 cm) suggesting a left-skew. However, these outliers appear unlikely for full-term babies and should be investigated further before proceeding with analysis.

**Modality** refers to the number of peaks or modes in a distribution. A unimodal distribution has one clear peak, bimodal has two peaks, and multimodal has more than two. Bi- or multimodal distributions indicate the presence of important subgroups in data that need to be taken into account when analysing data.

```{r stat-hist2, fig.cap='Histogram of radiation doses used for renography.'}
hist(reno$dose)
```

Figure \@ref(fig:stat-hist2) shows an example of bimodal data, suggesting the presence of two distinct groups of renography procedures -- each likely driven by a different underlying cause that is unknown to us. Before plotting these data -- as we did in Chapter \@ref(limits) -- we should first seek an explanation for the observed pattern and consider stratifying the data according to the underlying procedural strategy.

### Spread

The concept of spread in data is fundamental to SPC, as we have discussed numerous times throughout this book -- although the term "spread" itself has not been explicitly introduced until now. Spread refers to the variability or diversity within the data, indicating how much the values differ from each other.

When constructing control limits, we rely on the standard deviation (SD or sigma) of common cause variation as a primary measure of spread. However, standard deviation is not the only useful indicator. As demonstrated above, the summary() function provides additional measures that reflect the spread of data.

The range represents the distance between the highest and lowest values in a dataset, while the interquartile range (IQR) captures the spread of the middle 50% of the data â€” calculated as the difference between the first and third quartiles.

The **range** is easy to understand but has limited usefulness on its own, as it tends to increase with sample size. As the number of observations grows, so does the likelihood of including rare -- though not necessarily unusual -- values.

The **interquartile range**  is a more robust measure of spread and can also offer some insight into the shape of the distribution -- indirectly, through the position of the centre relative to the quartiles. However, it becomes unreliable with very small sample sizes. When fewer than five values are present -- at the very least -- calculating quartiles is effectively meaningless.

A useful and compact alternative to histograms is the box plot, which provides a visual summary of the central tendency and spread of a dataset.

```{r stat-box1, fig.cap='Boxplot of birth lengths.'}
boxplot(births$length)
```

The box in a box plot represents the interquartile range (IQR), while the line inside the box indicates the median. The whiskers extend from either end of the box to the smallest and largest values that lie within 1.5 times the IQR from the quartiles. Any dots beyond the whiskers represent more extreme values -- often referred to as outliers, though there is often nothing truly outlandish about them.

As long as data have a unimodal shape, the box plot is a very effective visualisation of both the centre, shape and spread of a distribution.

Box plots are especially useful with grouped data like in Figure \@ref(fig:stat-box2), which compares the birth length of boys and girls. The plot indicates that boys are -- on average -- slightly longer than girls.

```{r stat-box2, fig.cap='Stratified boxplot.'}
boxplot(length ~ sex, births)
```

The **standard deviation** is perhaps the most commonly used measure of spread in a dataset. It tells, on average, how far from the mean each data value lies.

To calculate SD we use 

$$
SD = \sqrt{\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n-1}}
$$

That is, the square root of the sum of squared deviations from the mean divided by one less than the number of values.

SD can always be calculated, but its usefulness depends on the underlying distribution of the data. This is why we first introduced the range and interquartile range, which are always reliable measures of spread regardless of distribution.

When data follow a Gaussian (or normal) distribution -- which we will talk more about in the next section -- the standard deviation has some particularly useful properties:

Approximately

* 68% of data values fall within Â±1 SD of the mean,
* 95% fall within Â±2 SD, and
* 99.7% fall within Â±3 SD.

The first two rules depend heavily on the dataâ€™s underlying distribution. However, the third rule -- that "most" data points fall within Â±3 SD -- holds true for many common distributions. Without delving into technical details, it can be shown that for *any* unimodal distribution, more than 95% of the data lie within 3 standard deviations of the mean, and for *many* unimodal distributions, this proportion rises to around 98%.

This fact justifies the use of three-sigma limits in SPC charts regardless of the shape of data and cautions against adopting tighter limits solely to increase sensitivity.

## Theoretical distributions

- Gaussian (normal)
- Poisson
- Binomial