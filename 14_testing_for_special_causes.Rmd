---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Testing for Special Cause Variation {#special_causes}


```{r, echo=FALSE}
knitr::opts_chunk$set(fig.asp = 3/5,
                      dev     = 'svg')
library(kableExtra)

source('R/stdchart.R', local = knitr::knit_global())
```


```{r spc-fig2, fig.cap='Control chart with a large (2 SD) transient shift in data', echo=FALSE}
y2[13] <- y2[13] + 2

stdchart(y2)
```

```{r spc-fig3, fig.cap='Control chart with a minor (1 SD) sustained shift in data', echo=FALSE}
y3[16:24] <- y3[16:24] + 1

stdchart(y3)
```


### Tests based on sigma limits

* the 3-sigma test

* the Western Electric rules

### Runs analysis – tests based on the distribution of data points around the centre line

* unusually long runs

* unusually few crossings

## look mom, no control limits! – using runs analysis as stand-alone rules with run charts
    
    

## Two types of errors when using SPC

Classifying variation into common cause or special cause is the primary focus of statistical process control methodology. In practice, this classification is subject to two types of error which can be compared to an imperfect screening test that sometimes shows a patient has disease when in fact the patient is free from disease (false positive), or the patient is free from disease when in fact the patient has disease (false negative).

* Error 1: Treating an outcome resulting from a common cause as if it were a special cause and (wrongly) seeking to find a special cause, when in fact the cause is the underlying process. 

* Error 2: Treating an outcome resulting from a special cause as if it were a common cause and so (wrongly) overlooking the special cause.

Either mistake can cause losses. If all data were treated as special cause variation, this maximises the losses from mistake 1. And if all data were treated as common cause variation, this maximises the losses from mistake 2.  Unfortunately, in practice it is impossible to reduce both mistakes to zero. Shewhart concluded that it was best to make either mistake only rarely and that this depended largely upon the costs of looking unnecessarily for special cause variation.  Using mathematical theory, empirical evidence, and pragmatism, he argued that setting control limits to ± three standard deviations from the mean provides a reasonable balance between making the two types of mistakes.

The choice of three standard deviations ensures there is a relatively small chance that an investigation of special cause variation will be unfounded. It has been argued that while three standard deviations was an appropriate choice for manufacturing industry, it is not stringent enough for healthcare processes – and two standard deviations may be more appropriate. Lowering the control limits to, say two standard deviations, will increase the sensitivity of the control chart, but will also increase the chances of false alarms. The extent to which this is acceptable requires decision-makers to balance the total costs (e.g. time, money, human resources, quality, safety, reputation) of investigating (true or false) signals versus the costs of overlooking these signals (and so not investigating). In practice, this is a matter of judgment which varies with context. Nevertheless, in the era of big data in healthcare the issue of false alarms needs greater appreciation and attention (we discuss this later in book).

