---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.asp = 1/2,
                      echo = FALSE,
                      dev     = 'svg')

source('R/stdchart.R', local = knitr::knit_global())
set.seed(33)
y1 <- y2 <- y3 <- rnorm(24)
```

# Testing for Special Cause Variation {#testing}

So far we have mostly concerned ourselves with Shewhart's 3-sigma rule, one or more data points outside the control limits, to detect special cause variation. 

The 3-sigma rule is very effective to detect large shifts in data. Imagine data from a symmetric distribution, if data shifted three SDs upwards, the old upper control limit would be the new centre line and we would expect about half of future data points to lie above the upper control limit. If data only shifted one SD, the old upper control limit would be the new 2-sigma limit and we would expect about 2.5% of future data points to lie above this line (Figure \@ref(fig:testing-fig1)).

```{r testing-fig1, fig.cap='Control chart with progressive shifts in data'}
set.seed(4444444)
y <- rnorm(36, rep(0:2, each = 12))

stdchart(y, ra = F) +
  geom_vline(xintercept = 12.5, linetype = 3) +
  geom_vline(xintercept = 24.5, linetype = 3) +
  annotate('text', c(6, 18, 30), 4, 
           label = c('mean = 0', 'mean = 1', 'mean = 2'))
```

The performance of the 3-sigma rule has been studied extensively, but to make a long story short, it is most useful when looking for shift in the order of at least 1.5 - 2 SDs. Minor to moderate shifts may go unnoticed for long periods of time. 

In order to increase the sensitivity of SPC charts to minor shifts in data a large number of additional rules have been suggested. But before we get to these rules we will take a look at different types of patterns in data that often accompany special causes.

## Patterns of non-random variation in time series data

In the iconic Western Electric Handbook [@we1956] a large number of control chart patterns are described to help engineers interpret control charts. The idea being that certain patterns are often due to certain causes.

In our experience, the most common special cause patterns found in healthcare data are freaks, shift, and trends. 

### Freaks

A freak is one or few data points that are distinctly different from the rest (Figure \@ref(fig:spc-fig2)). Freaks are by definition transient in nature -- they appear and then go away.

```{r spc-fig2, fig.cap='Control chart with a large (2 SD) transient shift in data', echo=FALSE}

y2[13] <- y2[13] + 2

stdchart(y2)
```

Freaks are often caused by data or sampling errors, but may also be results of transient external forces acting on the process, for example temporary changes in patient case mix. Finally, freaks may simply be part of the natural process which once in a while is expected to produce extreme values simply by chance.

### Shifts

A shift is a sudden and sustained change in process centre (Figure \@ref(fig:spc-fig3)). Minor to moderate shifts may go unnoticed by the 3-sigma rule for long periods of time. Large shifts may initially present themselves as freaks.

```{r spc-fig3, fig.cap='Control chart with a minor (1 SD) sustained shift in data', echo=FALSE}
y3[16:24] <- y3[16:24] + 1

stdchart(y3)
```

Shifts are caused by sudden changes in process behaviour. In healthcare improvement we strive to induce shift in the desired direction by improving the structures and procedures that are behind data.

### Trends

A trend is a gradual change in process centre (Figure \@ref(fig:spc-fig4)). Note that in some texts a trend is more specifically a series of data points that are all larger or smaller than the previous data points. In this book, we use the term "trend" in the more general sense as a gradual change in one direction.

```{r spc-fig4, fig.cap='Control chart with a trend in data', echo=FALSE}
set.seed(22222)
y4 <- rnorm(24, seq(-2, 2, length.out = 24))

stdchart(y4)
```

Trends are seen when forces outside the usual process are pushing data consistently in one direction, and specifically when the push is continuous and the effect cumulative. Trends are commonly seen when improvements are implemented in a stepwise fashion, for example by being introduced incrementally in a large organisation. In such cases we may observe sequential shifts in data a the department level that sum up to a trend at the organisational level.

### Other unusual patterns

There are, of course, countless other types of "unusual" patterns that may show up in data. However, it is our experience from healthcare, that process improvement or deterioration is most commonly associated with either freaks, shifts, or trends and learning to recognise these patterns are helpful in finding the underlying causes of change.

That said, depending on the context and the purpose of using SPC it may be useful to look for other types of patterns in data. This is especially true for cyclic or seasonal data where specific patterns are repeated depending on the time of day, week, month, year etc. [@jeppegaard2023]. For example, excess mortality related to weekend admissions may not present itself as neither freaks, shifts, or trends, but may nonetheless be important to recognise.

## SPC rules

If patterns of special cause variation were always as clear as in the figures \@ref(fig:spc-fig2), \@ref(fig:spc-fig3), and \@ref(fig:spc-fig4) there would be no need for SPC. However, in reality, special causes often show up in more subtle ways. To help objectify the analysis we use SPC rules, that is statistical tests that signal the presence -- or rather, the likelihood -- of special cause variation.

We have already seen Shewhart's original 3-sigma rule in action (Figure \@ref(fig:spc-fig2)), but many more rules have been developed to help identify minor to moderate shifts, trends, and other specific patterns in data. It is tempting to simply apply all known rules to our data, but -- as always in statistics -- the more tests we apply, the more signals we get, and the more signals we get, the more signals are false alarms signalling special causes when really no special causes are present.

Thus, it is important to select as few rules as possible to minimise the risk of false alarms but at the same time select as many rules as necessary to identify as many true alarms as possible. More on this topic in the ["Two types of errors"](#two-types-of-errors) section later in this chapter.

Basically there are two types of test for special cause variation in SPC charts. Some tests are based on the distribution of data points relative to the sigma lines including 1- and 2-sigma lines, which are placed one and two SDs from the centre line respectively. And then there are tests that are based solely on the distribution of data points relative to the centre line.

We will restrict ourselves to two sets of rules that have been thoroughly studied and validated.

### Tests based on sigma limits

The best known tests for special cause variation are probably the Western Electric Rules (WE) described in the Statistical Quality Control Handbook [@we1956]. The WE rules consist of four simple tests that can be applied to control charts by visual inspection and are based on the identification of unusual patterns in the distribution of data points relative to the control and centre lines.

1. One or more points outside of the 3-sigma limits (Shewhart's original 3-sigma rule).

2. Two out of three successive points beyond a 2-sigma limit (two thirds of the distance between the centre line and the control line).

3. Four out of five successive points beyond a 1-sigma limit.

4. A run of eight successive points on one side of the centre line.

We leave it to the reader to apply WE rules #2-#4 to figures \@ref(fig:spc-fig3) and  \@ref(fig:spc-fig4).

The WE rules have proven their worth during most of a century. One thing to notice though is that the WE rules are most effective with control charts that have between 20 and 30 data points. With fewer data points, they lose sensitivity (more false negatives), and with more data points they lose specificity (more false positives).

### Runs analysis – tests based on the distribution of data points around the centre line

WE rule #4 is independent of the sigma limits. It is based on assumptions regarding the length of runs on either side of the centre line.  A run is defined as one of more successive data points on the same side of the centre line.

If we assume that the centre line splits data in two halves, the probability of any data point being above or below the centre is fifty-fifty. Similarly, the probability of any two neighbouring points being on the same or opposite side is fifty-fifty. 

By dichotomising data in runs of data points that are either below or above a certain value, for example the centre line, a whole branch of statistical theory called runs analysis lies at our feet. The basic idea is that the lengths and number of runs in random processes are governed by laws of nature and thus predictable within limits.

In short, when a process shifts or trends, runs tend to get longer and fewer. Consequently, we may base our runs tests on finding unusually long or unusually few runs. The question now is: what constitutes "unusually" long or or "unusually" few?

From practical experiments, some theoretical considerations, and years of practice [@anhoej2014; @anhoej2015] we suggest these two tests:

* **Unusually long runs**: A run is one or more consecutive data points on the same side of the centre line. Data points that fall on the centre line neither break nor contribute to the run. The upper 95% prediction limit for longest run in a random process is approximately $log_2(n)+3$ (rounded to the nearest integer), where $n$ is the number of useful data points. For example, in a run chart with 24 data points a run of *more* than `round(log2(24) + 3)` = `r round(log2(24) + 3)` would suggest a shift in the process.

* **Unusually few crossings**: Rather than counting the number of runs, we count the number of crossings, which by definition is one less than the number of runs. A crossing is when two consecutive data points are on opposite sides of the centre line. In a random process, the number of crossings follows a binomial distribution, $b(n-1,0.5)$. The lower 5% prediction limit for number of crossings is found using `qbinom(p = 0.05, size = n - 1, prob = 0.5)`. Thus, in a run chart with 24 useful data points, *fewer* than `qbinom(0.05, 24 - 1, 0.5)` = `r qbinom(0.05, 24 - 1, 0.5)` crossings would suggest that the process is shifting.

Figure \@ref(fig:spc-fig3) has 24 data points, the longest run consists of 12 data points (#13 - #24), and the line crosses the centre line 9 times. Since the longest run is longer than expected, we may conclude that there is reason to believe that the process is shifting.

In Figure \@ref(fig:spc-fig3) there are two long runs with 9 data points and only 5 crossings also suggesting that data are shifting.

Note that the tests themselves do not tell what type of patterns are in data not to mention what the underlying causes of shifts and trends are. That interpretation lies with the data analyst and the people working in the process.

Critical values for longest runs and number of crossings for 10-100 data points are tabulated in the Appendix \@ref(runs-limits).

Apart from being comparable in sensitivity and specificity to WE rules #2-#4 with 20-30 data points [@anhoej2018b], these runs rules have some advantages:

* They do not depend on sigma limits and thus are useful as stand-alone rules with run charts (more on run charts in the next section).

* They adapt dynamically to the number of available data points, and can be applied to charts with as few as 10 and up to indefinitely many data points without losing sensitivity or specificity.

In practice the runs rules may be used as alternatives to WE rules #2-#4 to help identify shifts and trends alongside the WE rule #1 for freaks.

## Charts without borders -- using runs analysis as stand-alone rules with run charts

As we saw in the previous section, some rules only depend on a single line representing the centre of data. We made the assumption that the centre line is positioned such that it splits the data in two equally sized halves. In many cases the process average is a good representation of this centre, but if data are skewed in any way, this may not be the case. However, if we use the median rather than the mean of data the assumption always holds true.

So what happens if we remove the sigma lines and use the median for the centre line? We get a **run chart**: one of the most useful inventions in quality improvement and control.

```{r spc-fig5, fig.cap='Run chart', echo=FALSE, warning=FALSE}
stdchart(y1, lcl = NA_real_, ucl = NA_real_, cl = median(y1))
```

Figure \@ref(fig:spc-fig5) is a run chart of the data from Figure \@ref(fig:spc-fig2). We notice that the centre line is a little different from the control chart because we have used the median rather than the mean. Consequently, data are now evenly distributed around the centre line with 12 data points on each side. The longest run have 3 data points (#13-#15 and #22-#24), and the data line crosses the centre line 13 times. Since there are no unusually long runs and not unusually few crossing we conclude that this process shows no signs of persistent shifts in data.

Figure \@ref(fig:spc-fig6) is a run chart with data from Figure \@ref(fig:spc-fig4) containing a shift. The runs analysis confirms our "analysis-by-eye" by finding two unusually long runs with 9 data points and unusually few crossings (4).

```{r spc-fig6, fig.cap='Run chart with a trend', echo=FALSE, warning=FALSE}
stdchart(y4, lcl = NA_real_, ucl = NA_real_, cl = median(y4))
```

In practice the two rules are two sides of the same coin -- when runs get longer, crossings get fewer and vice versa -- and either of them may signal non-random variation.

Run charts are a lot simpler to construct than control charts. They do not make assumptions about the distribution of data. And they are more sensitive to minor to moderate persistent shifts and trends than control charts are (using only the 3-sigma rule).

So why do we bother making control charts at all? In fact, we can only think of two reasons: 1) control charts quickly pick up large, possibly transient, shifts (freaks) that would go unnoticed by run charts, and 2) the control limits provide a good measure and a visual representation of the width of the natural process variation.

Luckily, we do not need to choose between run and control charts. In fact, they are good companions for use at different stages of quality improvement and control.

## A practical approach to SPC analysis

Before we move on, let us pause for a moment and remind ourselves: What are we using SPC charts for? As discussed earlier, we use SPC mainly for two distinctly different tasks: process improvement and process monitoring.

When **improving** processes we strive to create sustained shifts in the desired direction in data. Thus, shifts are expected. Since improvement often happens in small steps, and since we are after lasting shifts, runs analysis is our tool of choice.

When **monitoring** processes our goal is to maintain process behaviour and stability, and we are not expecting data to shift. On the other hand, we want to pick up any signals suggesting that the process is deteriorating as quickly as possible. For this, the 3-sigma rule is particularly useful. We may chose to combine the 3-sigma rule with *either* the rest of the WE rules *or* with the two runs rules to get better sensitivity to minor to moderate shifts and trends. However, we should be aware that adding tests comes with a price of more false alarms.

Regardless of the task at hand, we recommend to always begin any SPC analysis with the "assumption free" run chart using the median as centre line. If the runs rules find only non-random variation, it makes no sense to compute sigma limits based on assumptions regarding the location and dispersion of data, because these figures are meaningless when one or more shifts have already been identified. (A bit like ordering a CT scan when the diagnosis has already been confirmed by conventional X-ray.) In that case, we should rather be looking for special causes than concerning ourselves with calculating control limits.

Then, if our goal is process improvement, we continue with the run chart until we see significant and sustained improvement resulting in a new and better process. When this new process has stabilised itself and looks set to continue, we may consider moving into "monitoring mode" and add control limits to our chart.

Monitoring a process makes sense when the process is stable (common cause variation only) *and* satisfactory. If that is not the case, we should start over in "improvement mode".

## Two types of errors when using SPC {#two-types-of-errors}

Classifying variation into common cause or special cause is the primary focus of statistical process control methodology. In practice, this classification is subject to two types of error which can be compared to an imperfect screening test that sometimes shows a patient has disease when in fact the patient is free from disease (false positive), or the patient is free from disease when in fact the patient has disease (false negative).

* Error 1 (false positive): Treating an outcome resulting from a common cause as if it were a special cause and (wrongly) seeking to find a special cause, when in fact the cause is the underlying process. 

* Error 2 (false negative): Treating an outcome resulting from a special cause as if it were a common cause and so (wrongly) overlooking the special cause.

Either mistake can cause losses. If all data were treated as special cause variation, this maximises the losses from mistake 1. And if all data were treated as common cause variation, this maximises the losses from mistake 2.  Unfortunately, in practice it is impossible to reduce both mistakes to zero. Shewhart concluded that it was best to make either mistake only rarely and that this depended largely upon the costs of looking unnecessarily for special cause variation.  Using mathematical theory, empirical evidence, and pragmatism, he argued that setting control limits to ± three standard deviations from the mean provides a reasonable balance between making the two types of mistakes.

The choice of three standard deviations ensures there is a relatively small chance that an investigation of special cause variation will be unfounded. It has been argued that while three standard deviations was an appropriate choice for manufacturing industry, it is not stringent enough for healthcare processes – and two standard deviations may be more appropriate. Lowering the control limits to, say two standard deviations, will increase the sensitivity of the control chart, but will also increase the chances of false alarms. The extent to which this is acceptable requires decision-makers to balance the total costs (e.g. time, money, human resources, quality, safety, reputation) of investigating (true or false) signals versus the costs of overlooking these signals (and so not investigating). In practice, this is a matter of judgement which varies with context. Nevertheless, in the era of big data in healthcare the issue of false alarms needs greater appreciation and attention. We discuss this later in the book.

