---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.asp = 1/2,
                      echo = FALSE,
                      dev     = 'svg')

source('R/stdchart.R', local = knitr::knit_global())
set.seed(33)
y1 <- y2 <- y3 <- rnorm(24)
```

# Looking for signals of Special Cause Variation {#testing}

So far we have mostly concerned ourselves with Shewhart's 3-sigma rule, one or more data points outside the control limits, to detect special cause variation. 

The 3-sigma rule is very effective to signal large shifts in data. Imagine data from a normal distribution, if data shifted three SDs upwards, the old upper control limit would be the new centre line and we would expect about half of future data points to lie above the upper control limit. If data only shifted one SD, the old upper control limit would be the new 2-sigma limit and we would expect about 2.5% of future data points to lie above this line (Figure \@ref(fig:testing-fig1)).

```{r testing-fig1, fig.cap='Control chart with progressive shifts in data'}
set.seed(4444444)
y <- rnorm(36, rep(0:2, each = 12))

stdchart(y, ra = F) +
  geom_vline(xintercept = 12.5, linetype = 3) +
  geom_vline(xintercept = 24.5, linetype = 3) +
  annotate('text', c(6, 18, 30), 4, 
           label = c('mean = 0', 'mean = 1', 'mean = 2'))
```

The performance of the 3-sigma rule has been studied extensively, but to make a long story short, it is most useful when looking for shift in the order of at least 1.5 - 2 SDs. Minor to moderate shifts may go unnoticed for long periods of time. 

In order to increase the sensitivity of SPC charts to minor shifts in data a large number of additional rules have been suggested. But before we get to these rules we will take a look at different types of patterns in data that often accompany special causes.

## Patterns of non-random variation in time series data

In the iconic Western Electric Handbook [@we1956] a large number of control chart patterns are described to help engineers interpret control charts. The idea being that certain patterns are often related to certain causes.

In our experience, the most common special cause patterns found in healthcare data are freaks, shifts, and trends. 

### Freaks

A freak is one or few data points that are distinctly different from the rest (Figure \@ref(fig:testing-fig2)). Freaks are by definition transient in nature -- they appear and then go away.

```{r testing-fig2, fig.cap='Control chart with a large (2 SD) transient shift in data', echo=FALSE}

y2[13] <- y2[13] + 2

stdchart(y2)
```

Freaks are often caused by data or sampling errors, but may also be results of transient external forces acting on the process, for example temporary changes in patient case mix during a holiday season. Finally, freaks may simply be part of the natural process which once in a while is expected to produce extreme values simply by chance (false alarm).

### Shifts

A shift is a sudden and sustained change in process centre (Figure \@ref(fig:testing-fig3)). Minor to moderate shifts may go unnoticed by the 3-sigma rule for long periods of time. Large shifts may initially present themselves as freaks.

```{r testing-fig3, fig.cap='Control chart with a minor (1 SD) sustained shift in data', echo=FALSE}
y3[16:24] <- y3[16:24] + 1

stdchart(y3)
```

Shifts are caused by sudden changes in process behaviour. In fact, in healthcare improvement we strive to induce shifts in the desired direction by improving the structures and procedures that are behind data.

### Trends

A trend is a gradual change in process centre (Figure \@ref(fig:testing-fig4)). Note that in some texts a trend is more specifically a series of data points that are all going up or down. In this book, we use the term "trend" in the more general sense as a gradual change in one direction.

```{r testing-fig4, fig.cap='Control chart with a trend in data', echo=FALSE}
set.seed(22222)
y4 <- rnorm(24, seq(-2, 2, length.out = 24))

stdchart(y4)
```

Trends are seen when forces outside the usual process are pushing data consistently in one direction, and specifically when the push is continuous and the effect cumulative. Trends are commonly seen when improvements are implemented in a stepwise fashion, for example by being introduced incrementally in a large organisation. In such cases we may observe sequential shifts in data a the department level that sum up to a trend at the organisational level.

### Other unusual patterns

There are, of course, countless other types of "unusual", not to mention mixed, patterns that may show up in data. However, it is our experience from healthcare, that process improvement or deterioration is most commonly associated with either freaks, shifts, or trends and learning to recognise these patterns is helpful in finding the underlying causes of change.

That said, depending on the context and the purpose of using SPC it may be useful to look for other types of patterns in data. This is especially true for cyclic or seasonal data where specific patterns are repeated depending on the time of day, week, month, year etc. [@jeppegaard2023]. For example, excess mortality related to weekend admissions may not present itself as neither freaks, shifts, or trends, but may nonetheless be important to recognise.

## SPC rules

If patterns of special cause variation were always as clear as in the figures \@ref(fig:testing-fig2), \@ref(fig:testing-fig3), and \@ref(fig:testing-fig4) there would be no need for SPC. However, in reality, special causes often show up in more subtle ways. To help objectify the analysis we use SPC rules, that is statistical tests that signal the presence -- or rather, the likelihood -- of special cause variation.

We have already seen Shewhart's original 3-sigma rule in action, but many more rules have been developed to help signal minor to moderate shifts, trends, and other specific patterns in data. It is tempting to simply apply all known rules to our data, but -- as always in statistics -- the more tests we apply, the more signals we get, and the more signals we get, the more signals are false alarms signalling special causes when really only common cause is present.

Thus, it is important to select as few rules as possible to minimise the risk of false alarms but at the same time select as many rules as necessary to identify as many true alarms as possible. More on this topic later in the chapter on [Diagnostic Errors](#diagnostics) later in this book.

Basically there are two types of tests for signals of special cause variation in SPC charts. Some tests are based on the distribution of data points relative to the sigma lines including 1- and 2-sigma lines, which are placed one and two SDs from the centre line respectively. And then there are tests that are based solely on the distribution of data points relative to the centre line.

We will restrict ourselves to two sets of rules that have been thoroughly studied and validated.

### Tests based on sigma limits

The best known tests for special cause variation are probably the Western Electric Rules (WE) described in the Statistical Quality Control Handbook [@we1956]. The WE rules consist of four simple tests that can be applied to control charts by visual inspection and are based on the identification of unusual patterns in the distribution of data points relative to the control and centre lines.

1. One or more points outside of the 3-sigma limits (Shewhart's original 3-sigma rule).

2. Two out of three successive points beyond a 2-sigma limit (two thirds of the distance between the centre line and the control line).

3. Four out of five successive points beyond a 1-sigma limit.

4. A run of eight successive points on one side of the centre line.

We leave it to the reader to apply WE rules #2-#4 to figures \@ref(fig:testing-fig3) and  \@ref(fig:testing-fig4).

The WE rules have proven their worth during most of a century. One thing to notice though is that the WE rules are most effective with control charts that have between 20 and 30 data points. With fewer data points, they lose sensitivity (more false negatives), and with more data points they lose specificity (more false positives).

### Runs analysis â€“ tests based on the distribution of data points around the centre line

WE rule #4 is independent of the sigma limits. It is based on assumptions regarding the length of runs on either side of the centre line.  A run is defined as one of more successive data points on the same side of the centre line.

If we assume that the centre line splits data in two halves, the probability of any data point being above or below the centre is fifty-fifty. Similarly, the probability of any two neighbouring points being on the same or opposite side is fifty-fifty. 

By dichotomising data in runs of data points that are either below or above a certain value, for example the centre line, a whole branch of statistical theory called runs analysis lies at our feet. The basic idea is that the lengths and number of runs in random processes are governed by laws of nature and thus predictable within limits.

In short, when a process shifts or trends, runs tend to get longer and fewer. Consequently, we may base our runs tests on finding unusually long or unusually few runs. The question now is: what constitutes "unusually" long and "unusually" few?

From practical experiments, some theoretical considerations, and years of practice [@anhoej2014; @anhoej2015] we suggest these two runs tests:

* **Unusually long runs**: A run is one or more consecutive data points on the same side of the centre line. Data points that fall directly on the centre line neither break nor contribute to the run. The upper 95% prediction limit for longest run in a random process is approximately $log_2(n)+3$ (rounded to the nearest integer), where $n$ is the number of useful data points (data points not on the centre line). For example, in a run chart with 24 useful data points a run of *more* than `round(log2(24) + 3)` = `r round(log2(24) + 3)` would suggest a shift in the process.

* **Unusually few crossings**: Rather than counting the number of runs, we count the number of crossings, which by definition is one less than the number of runs. A crossing is when two consecutive data points are on opposite sides of the centre line. In a random process, the number of crossings follows a binomial distribution. The lower 5% prediction limit for number of crossings is found using the cumulative probability distribution, `qbinom(p = 0.05, size = n - 1, prob = 0.5)`. Thus, in a run chart with 24 useful data points, *fewer* than `qbinom(0.05, 24 - 1, 0.5)` = `r qbinom(0.05, 24 - 1, 0.5)` crossings would suggest that the process is shifting.

Figure \@ref(fig:testing-fig3) has 24 data points, the longest run consists of 12 data points (#13 - #24), and the line crosses the centre line 9 times. Since the longest run is longer than expected, we may conclude that there is reason to believe that the process is shifting.

In Figure \@ref(fig:testing-fig3) there are two long runs with 9 data points and only 5 crossings also suggesting that data are shifting.

Note that the tests themselves do not tell what type of patterns are in data not to mention what the underlying causes of shifts and trends are. That interpretation lies with the data analyst and the people working in the process.

Critical values for longest runs and number of crossings for 10-100 data points are tabulated in Appendix \@ref(runs-limits).

Apart from being comparable in sensitivity and specificity to WE rules #2-#4 with 20-30 data points [@anhoej2018b], these runs rules have some advantages:

* They do not depend on sigma limits and thus are useful as stand-alone rules with run charts (more on run charts in the next section).

* They adapt dynamically to the number of available data points, and can be applied to charts with as few as 10 and up to indefinitely many data points without losing sensitivity or specificity.

In practice these runs rules may be used as alternatives to WE rules #2-#4 to help identify shifts and trends alongside the WE rule #1 for freaks.

## Charts without borders -- using runs analysis as stand-alone rules with run charts

As we saw in the previous section, some rules only depend on a single line representing the centre of data. We made the assumption that the centre line is positioned such that it splits the data in two equally sized halves. In many cases the process average is a good representation of this centre, but if data are skewed in any way, this may not be the case. However, if we use the median rather than the mean of data the assumption always holds true.

So what happens if we remove the sigma lines and use the median for the centre line? We get a **run chart**: one of the most useful inventions in quality improvement and control.

```{r testing-fig5, fig.cap='Run chart', echo=FALSE, warning=FALSE}
stdchart(y1, lcl = NA_real_, ucl = NA_real_, cl = median(y1))
```

Figure \@ref(fig:testing-fig5) is a run chart of the data from Figure \@ref(fig:spc-fig1). We notice that the centre line is a little different from the control chart because we have used the empirical median rather than the theoretical mean. Consequently, data are now evenly distributed around the centre line with 12 data points on each side. The longest run have 3 data points (#13-#15 and #22-#24), and the data line crosses the centre line 13 times. Since there are no unusually long runs and not unusually few crossing we conclude that this process shows no signs of persistent shifts in data.

Figure \@ref(fig:testing-fig6) is a run chart with data from Figure \@ref(fig:testing-fig4) containing a trend. The runs analysis confirms our "analysis-by-eye" by finding two unusually long runs with 9 data points and unusually few crossings (4).

```{r testing-fig6, fig.cap='Run chart with a trend', echo=FALSE, warning=FALSE}
stdchart(y4, lcl = NA_real_, ucl = NA_real_, cl = median(y4))
```

The two runs rules are two sides of the same coin -- when runs get longer, crossings get fewer and vice versa -- and either of them may signal special cause variation.

Run charts are a lot simpler to construct than control charts. They do not make assumptions about the distribution of data. And they are more sensitive to minor to moderate *persistent* shifts and trends than control charts are (using only the 3-sigma rule).

So why do we bother making control charts at all? In fact, we can only think of two reasons: 1) control charts quickly pick up large, possibly transient, shifts (freaks) that would go unnoticed by run charts, and 2) the control limits provide a good measure and a visual representation of the width of the natural process variation.

Luckily, we do not need to choose between run and control charts. In fact, they are good companions for use at different stages of quality improvement and control.

## A practical approach to SPC analysis

Before we move on, let us pause for a moment and remind ourselves: What are we using SPC charts for? As discussed earlier, we use SPC mainly for two distinctly different tasks: process improvement and process monitoring.

When **improving** processes we strive to create sustained shifts in the desired direction in data. Thus, shifts are expected. Since improvement often happens in small steps, and since we are after lasting shifts, runs analysis is our tool of choice.

When **monitoring** processes our goal is to maintain process behaviour and stability, and we are not expecting data to shift. On the other hand, we want to pick up any signals suggesting that the process is deteriorating as quickly as possible. For this, the 3-sigma rule is particularly useful. We may chose to combine the 3-sigma rule with *either* the rest of the WE rules *or* with the two runs rules to get better sensitivity to minor to moderate shifts and trends. However, we should be aware that adding tests comes with a price of more false alarms.

Regardless of the task at hand, **we recommend to always begin any SPC analysis with the "assumption free" run chart** using the median as centre line. If the runs analysis signals non-random variation, it makes no sense to compute sigma limits based on assumptions regarding the location and dispersion of data, because these figures are meaningless when one or more shifts have already been identified. (A bit like ordering a CT scan when the diagnosis has already been confirmed by conventional X-ray.) In that case, we should rather be looking for special causes than concerning ourselves with calculating control limits.

Then, if our goal is process improvement, we continue with the run chart until we see significant and sustained improvement resulting in a new and better process. 

If and when this new better process stabilises and looks set to continue, we may consider moving into monitoring mode by adding control limits and using the mean for centre line.

Monitoring a process makes sense when the process is stable (common cause variation only) *and* satisfactory. If that is not the case, we should start over in improvement mode.

## SPC rules in summary

SPC rules are statistical tests designed to identify signs of special cause variation (signals) in time series data. Depending on specific patterns in data suggesting special causes many rules have been suggested. However, as more and more rules are applied, the risk of false alarms increases. Consequently, it pays to select a small battery of rules that together maximises the chance of signalling special causes that are there (true alarms) while minimising the risk of false alarms.

We recommend to always begin any SPC analysis with a median-based run chart and the two runs rules testing for unusually long runs and/or unusually few crossings. If -- and only if -- the process is stable and satisfactory, we may chose to use the mean for centre line and to add control limits to help identify freaks and other sudden, large shifts i data.
