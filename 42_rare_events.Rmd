---
output: html_document
editor_options: 
  chunk_output_type: console
---
# SPC Charts for Rare Events

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.asp = 2/4,
                      dev     = 'svg')

library(qicharts2)
```

When dealing with potentially serious or fatal events, the number of occurrences is often (fortunately) very low, which can present challenges for traditional SPC charts designed for count data.

The challenge arises because traditional SPC charts for count data -- like P and U charts -- assume a relatively higher and more consistent frequency of events to function effectively. When events are rare, the data becomes sparse and highly variable, making these charts unreliable:

* Too many zeros: Frequent zero counts can lead to control limits that are misleadingly narrow or wide. Especially, with run charts, when more than half of the data points are zero, the median will also be zero, which undermines the validity of the runs analysis -- typically resulting in just one long run above the median.

* Low sensitivity: Traditional charts may fail to detect meaningful shifts or trends because the event frequency is too low to produce statistically significant signals.

* False alarms or missed signals: The charts may either trigger false alarms due to natural variation in rare data or miss actual process shifts, leading to poor decision-making.

One useful approach to handling rare events is to plot the number of opportunities or the time between events, rather than focusing on event proportions or rates -- essentially flipping the indicator to look at the gaps between occurrences instead of the occurrences themselves. 

Another approach is to look at the cumulated sums (CUSUM) of binary data.

In this chapter we introduce the G chart for number of opportunities between events, the T chart for time between events, and the Bernoulli CUSUM chart for binary data.

## Introducing the birth dataset

The [Robson group 1 births] dataset is a data frame with 2193 observations from Robson group 1 deliveries, that is: first time pregnancy, single baby, head first, gestational age at least 37 weeks.

```{r}
# import data
births <- read.csv('data/robson1_births.csv',
              comment.char = '#',
              colClasses   = c('POSIXct',
                               'Date',
                               'logical',
                               'logical',
                               'integer',
                               'integer',
                               'integer',
                               'double',
                               'logical'))

# make sure the rows are sorted in time order
births <- births[order(births$datetime), ]
# add dummy variable, case, for counting the denominator for the proportion charts
births$case <- 1
# show data structure
str(births)
```

For this chapter we are interested in the asphyxia variable, which is a logical vector that is TRUE when the baby had either low apgar score or low umbilical cord pH suggesting lack of oxygen during delivery. Asphyxia is a very serious condition that may result in permanent brain damage or death. In total, there are `r sum(births$asphyxia)` cases corresponding to `r paste0(round(mean(births$asphyxia) * 100, 1), '%')`.

Figure \@ref(fig:rare-fig1) is a run chart of the biweekly counts.

```{r rare-fig1, fig.cap='Run chart of number of deliveries with neonatal asphyxia'}
# run chart of counts
qic(biweek, asphyxia, 
    data    = births, 
    agg.fun = 'sum')  # use sum() function to aggregate data by subgroup
```

Notice that because more than half the data points are zero, the centre line (median) is also zero. This invalidates the runs analysis -- there is only one very long run, which may lead to the false conclusion that data contains one or more shifts.

Figure \@ref(fig:rare-fig3) plots proportions rather than counts reaching the same conclusion.

```{r rare-fig2, fig.cap='Run chart of proportion deliveries with neonatal asphyxia'}
# run chart of proportions
qic(biweek, asphyxia, case,  # use the case variable for denominators
    data = births)
```

Plotting the data on a P chart provides control limits and allows for a more meaningful runs analysis -- as the average, in this case, better represents the process centre (figure \@ref(fig:rare-fig3)).

```{r rare-fig3, fig.cap='P chart of percent deliveries with neonatal asphyxia'}
# P chart
qic(biweek, asphyxia, case, 
    data  = births, 
    chart = 'p')
```

While this P chart remains useful, the lower control limit is zero, making it impossible to detect a signal of improvement using the three-sigma rule. As a result, we must rely solely on runs analysis to identify improvement.

One obvious solution is to increase the subgroup size by aggregating data over longer periods, such as months or quarters. However, this approach results in slower detection of process improvement or deterioration.

As an alternative, we introduce at the G chart.

## G charts for opportunities between cases

The G chart plots opportunities between cases, for example the number of deliveries between neonatal asphyxia. This type of count data often follows a geometric distribution with a standard deviation given by $\sigma = \sqrt{\bar{x}(\bar{x}+1)}$. The control limits are then calculated as:

$$\bar{x}\pm3\sqrt{\bar{x}(\bar{x}+1)}$$

where $\bar{x}$ is the average number of opportunities between cases.

To obtain the number-between variable, we calculate the differences between the indices of cases (remember to first sort the data in time order).

```{r rare-fig4, fig.cap='G chart of deliveries between neonatal asphyxia'}
# get indices of asphyxia cases
asph_cases <- which(births$asphyxia)
# calculate the number of deliveries between cases
asph_g     <- diff(c(asph_cases))

# plot G chart
qic(asph_g, chart = 'g')
```

Since the geometric distribution is highly skewed, the average is not ideal for runs analysis, which assumes the data are symmetrically distributed around the centre. Instead, the median may be used -- as is done by default in qicharts2 (\@ref(fig:rare-fig4)).

As is the case with the other SPC charts for counts data, negative lower control limits are rounded to 0, as values below this is not possible.

Note that process improvement -- as in fewer cases -- will present itself as the curve going up. To trigger a 3-sigma signal we would -- in this case -- need at least 518 successive deliveries without asphyxia.

Thus, G charts are useful alternatives to P charts when occurrences are rare and the primary interest is in detecting process improvement. However, one chart does not exclude the other, and P and G charts go well together. The P chart may be more familiar to users and is more useful for signalling process deterioration, while the G charts helps trigger signals of improvement.

## T charts for time between events

The T chart plots the time between events -- a continuous variable. While it is possible to use an I chart to plot such data, this approach may be problematic. If events occur according to a Poisson distribution -- which is often the case -- the time between events is more appropriately modelled by the exponential distribution, which is highly skewed.

One way to address this issue is to transform the data prior to plotting it on an I chart. An appropriate transformation is given by:

$$x = y^{(1 / 3.6)}$$

where $x$ is the transformed variable and $y$ is the original time between events variable.

Control limits can then be calculated for the transformed data using the standard I chart procedure.

However, the transformed values may be difficult to interpret. A better approach is to plot the original values alongside back-transformed control limits and centre line. The back-transformation is:

$$y=x^{3.6}$$.

```{r rare-fig5, fig.cap='T chart of time (days) between deliveries with neonatal asphyxia'}
# get time between events
asph_t <- diff(c(births$datetime[asph_cases]))
# plot T chart
qic(asph_t, chart = 't')
```

Note that T charts do not accommodate zero time between events -- a situation that may arise when time is not recorded with sufficiently high resolution. For example, if two events -- such as patient falls -- occur on the same day and the time of day is not recorded, the time variable is not truly continuous. In such cases, a G chart plotting days between events, a discrete variable, may be more appropriate.

In fact, to our eyes, the control limits on the T chart in Figure \@ref(fig:rare-fig5) appear unnaturally wide. The upper control limit indicates that at least 190 days (or 27 weeks) must pass without any asphyxia cases before a signal is triggered. This suggests to us that these asphyxia cases (being discrete cases rather than random events) may not be suitable for analysis with a T chart.

## The Bernoulli CUSUM chart for binary data

The Bernoulli CUSUM chart (also known as the B chart) is a specialized control chart used to detect small shifts in the proportion of binary outcomes (e.g., pass/fail, success/failure) over time [@neuburger2017]. It operates on a logical vector (TRUE/FALSE), where each observation contributes to a trace statistic, computed as the cumulative sum of deviations from a predefined target proportion.

The trace value $s_i$ at time $i$ is updated recursively:

$$s_i=s_{i-1}+ increment\ based\ on\ current\ value$$

When the current observation $x_i$ is TRUE (i.e., an event of interest occurs), the trace increases; when $x_i$ is FALSE, the trace decreases.

The size of each increment is determined by both the target proportion and the magnitude of the shift the chart is designed to detect. This design allows the B chart to accumulate evidence over time, enhancing sensitivity to sustained deviations from the target.

If $x_i$ = TRUE:

$$s_i=s_{i-1}+logOR-log(1+p_0(OR-1))$$

If $x_i$ = FALSE:

$$s_i=s_{i-1}-log(1-p_0(OR-1))$$

Here:

* $s_i$ is the CUSUM statistic at time i
* $p_0$ is the target (or baseline) proportion
* $OR$ is the odds ratio representing the smallest shift the chart should detect

For example, setting OR = 2 configures the chart to detect a doubling of the target proportion p~0~.

If the trace remains near zero, it means that the process is operating close to target.

```{r rare-fig6, fig.cap='B chart of newborns with asphyxia'}
bchart(births$asphyxia, target = 0.007, or = 2, limit = 3.5)
```

Figure \@ref(fig:rare-fig6) places the asphyxia data in a B chart constructed using the bchart() function from the qicharts2 package. The target value corresponds to the known baseline proportion of asphyxia cases with default values for odds ratio (= 2) and limits (= ±3.5).

Several key features are worth noting:

* The chart displays two cumulative traces: the upper trace is configured to detect an increase from the target proportion -- specifically, a doubling in odds (OR = 2) -- while the lower trace is tuned to detect a decrease (OR = 0.5).

* The trace values have no intrinsic meaning beyond their directional movement -- they increase in response to TRUE values and decrease in response to FALSE values. The magnitude of the trace simply reflects the cumulative deviation from the target, not the severity or frequency of events in absolute terms.

* Traces reset to zero whenever they cross the horizontal axis or a control limit, allowing the chart to remain responsive to new patterns of deviation.

* The control limits are user-defined, rather than statistically derived. Their placement represents a compromise between desired sensitivity (the ability to detect true shifts) and specificity (resistance to false alarms). By default, the limits are set at ±3.5. More on how to chose limits later.

Figure \@ref(fig:rare-fig8) compares our results against a target proportion of 2%.

```{r rare-fig8, fig.cap='B chart of newborns with asphyxia, target = 0.2%'}
bchart(births$asphyxia, target = 0.02)
```

The lower trace signals repeatably suggesting that the current process is not centred around a target of 2% -- specifically, it indicates that the process is performing better than expected.

And vice versa, comparing to a target of 0.1% indicates that the process is performing worse than expected (Figure \@ref(fig:rare-fig9)).

```{r rare-fig9, fig.cap='B chart of newborns with asphyxia, target = 0.01%'}
bchart(births$asphyxia, target = 0.001)
```

Control limits for B charts are user-defined and reflect a trade-off between sensitivity and specificity. A common default is ±3.5, suitable for detecting a doubling or halving of the event rate. Tighter limits increase sensitivity but may lead to more false alarms; wider limits reduce false positives but may miss smaller shifts. The best choice depends on context, such as the number of available observations, size of the shift to be detected, and the consequences of missing a signal or triggering a false one.

In practice, limits are often chosen based on simulation, historical data, and domain knowledge -- particularly considering the cost or impact of missed detections versus false alerts. See @neuburger2017 for details and guidelines on how to select control limits.

Inspecting Figure \@ref(fig:rare-fig6), which shows a historically stable process based on 2000+ observations and with most or all data points clustered around the centre line, we might conclude that it would be appropriate to tighten the control limits to ±3 or even ±2.5, as demonstrated in Figure \@ref(fig:rare-fig10).

```{r rare-fig10, fig.cap='B chart of newborns with asphyxia, target = 0.01%'}
bchart(births$asphyxia, target = 0.007, limit = 2.5)
```

## Selecting the rigth chart for rare events

After reviewing various alternative charting options for rare events -- U and T charts for low event rates, and P, G, and B charts for low case proportions -- we may ask: which should we prefer? Well, that depends. No single chart is universally superior -- and often, presenting two or more charts together can provide complementary insights. In particular, we always recommend pairing a T or G chart with its corresponding U or P chart, as these pairs may offer a more complete picture of the process by highlighting both improvement and deterioration.

The B chart, in particular, is a powerful tool, but it requires careful attention and specialist knowledge to construct and interpret effectively. Its diagnostic performance is highly sensitive to the choice of parameters — target, odds ratio, and control limits -- and these should be selected by individuals with a strong understanding of the underlying process. That said, the default settings -- an odds ratio of 2 and control limits at ±3.5 -- are generally suitable for most processes where the baseline (target) rate is around 1%, and where the goal is to detect a doubling or halving of the event rate relative to that target.




